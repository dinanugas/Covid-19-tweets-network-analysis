{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "setup notebook"
      ],
      "metadata": {
        "id": "MsKxnfPJYhvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "U7xoBaP9Xzac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# buka file dataset dari gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "link_folder = '/content/gdrive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONEtnL7YeSP",
        "outputId": "d92149f4-b067-4c18-fba5-e2f2b6830d5c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA (Exploratory Data Analysis)\n"
      ],
      "metadata": {
        "id": "01J51Racmh6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "references:\n",
        "- https://towardsdatascience.com/community-detection-algorithms-9bd8951e7dae\n",
        "- https://medium.com/@marcosacj/creating-and-visualizing-a-complex-network-of-instagram-hashtags-based-on-posts-about-politics-2daf24f31088\n",
        "- https://github.com/marcosacj/cna-instagram/blob/master/2019-06-11-1-macj-graph-scratch.ipynb\n",
        "- https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JLL5GOA_xc86",
        "outputId": "23d0b46b-79f3-40ca-91fa-78de02befb7b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nreference:\\n- https://towardsdatascience.com/community-detection-algorithms-9bd8951e7dae\\n- https://github.com/marcosacj/cna-instagram/blob/master/2019-06-11-1-macj-graph-scratch.ipynb\\n- https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new covid 19 twitter dataset from a"
      ],
      "metadata": {
        "id": "LjSBC-PxMu76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv( link_folder+'Covid-19 Twitter Dataset (Apr-Jun 2020).csv' )\n",
        "# df = pd.read_csv( link_folder+'Covid-19 Twitter Dataset (Apr-Jun 2021).csv' )\n",
        "filtered_df = df[ df['hashtags'].notnull() ].copy()\n",
        "\n",
        "# # check unique values from attributes from df\n",
        "# print( \"Unique values from category column: \", sorted( df['hashtags'].unique() ), \", count: \", df['hashtags'].nunique() )\n",
        "\n",
        "# preview data from file\n",
        "# df.head()\n",
        "print( \"origin data number: \", df.shape[0] )\n",
        "print( \"filtered data number: \", filtered_df.shape[0] )\n",
        "\n",
        "filtered_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mli6xducMvay",
        "outputId": "15190748-6ebf-42c6-a8af-583c331e1fa9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin data number:  143903\n",
            "filtered data number:  28510\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 28510 entries, 4 to 143899\n",
            "Data columns (total 17 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               28510 non-null  float64\n",
            " 1   created_at       28510 non-null  object \n",
            " 2   source           28500 non-null  object \n",
            " 3   original_text    28510 non-null  object \n",
            " 4   lang             28510 non-null  object \n",
            " 5   favorite_count   28510 non-null  float64\n",
            " 6   retweet_count    28510 non-null  float64\n",
            " 7   original_author  28510 non-null  object \n",
            " 8   hashtags         28510 non-null  object \n",
            " 9   user_mentions    20999 non-null  object \n",
            " 10  place            21231 non-null  object \n",
            " 11  clean_tweet      28454 non-null  object \n",
            " 12  compound         28510 non-null  float64\n",
            " 13  neg              28510 non-null  float64\n",
            " 14  neu              28510 non-null  float64\n",
            " 15  pos              28510 non-null  float64\n",
            " 16  sentiment        28510 non-null  object \n",
            "dtypes: float64(7), object(10)\n",
            "memory usage: 3.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags_on_posts = filtered_df['hashtags'].unique()\n",
        "number_current_hastags_pairs = filtered_df['hashtags'].nunique()\n",
        "\n",
        "# get list of one hashtags from posts\n",
        "def get_hashtags_list( posts ):\n",
        "  # initial value\n",
        "  final_list_tags = []\n",
        "  final_list_posts = []\n",
        "  max_posts = 100\n",
        "\n",
        "  # loop tiap satu data dari hashtags, misal limit dulu ke 1000 post\n",
        "  for all_hashtags in posts[ :max_posts ]:\n",
        "    # divide hashtags into one list\n",
        "    array_hashtags = all_hashtags.lower().split( \",\" )\n",
        "\n",
        "    # save current hastags list from one post\n",
        "    final_list_posts.append( array_hashtags )\n",
        "\n",
        "    # loop tiap hashtags\n",
        "    for hash in array_hashtags:\n",
        "      # normalsasi penulisan hashtags dalam bentuk lowercase untuk meminimalkan data double\n",
        "      if hash.lower() not in final_list_tags: final_list_tags.append( hash.lower() )\n",
        "  # end loop\n",
        "\n",
        "  # return final list\n",
        "  return final_list_tags, final_list_posts\n",
        "# end function\n",
        "\n",
        "# testing function\n",
        "# varitions_of_hashtags, list_of_hashtags_from_each_post = get_hashtags_list( hashtags_on_posts[0:100] )\n",
        "varitions_of_hashtags, list_of_hashtags_from_each_post = get_hashtags_list( hashtags_on_posts )\n",
        "print( varitions_of_hashtags )\n",
        "print( 'banyak jenis hashtags: ', len( varitions_of_hashtags ) )\n",
        "print( 'banyak posts: ', len( list_of_hashtags_from_each_post ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axyfRPpyXlNk",
        "outputId": "486254ac-9c15-4c30-9e7b-2f59734a4fd5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['covid_19', 'coronavirus', 'covid19', 'oic', ' islamophobic', ' india', ' covid', 'quarantinewine', 'abhealth', ' covid19ab', ' medicareforall', 'openamerica', 'frontlineheroes', 'togetherathome', 'covid', 'worthlessapologies', 'backboris', 'plasma', ' hmc', ' yoursafetyismysafety', ' coronavirus', ' covid_19', ' qatar', 'physicaldistancing', ' selfisolation', 'trump', ' timscott', '60minutes', ' covid19', ' environment', 'live', 'saheart', ' movemoremonth', ' movemore', 'howwefeel', ' demcastoh', 'cityofto', 'covid__19', ' cuba', 'blackaf', 'lagos', 'cloud28', ' covid__19', ' china', 'crypto', ' malware', ' cybersecurity', ' infosec', ' infosecurity', 'singapore', 'peanutbutter', ' asymptomatic', ' carriers', ' covidtesting', ' testing', ' senseofsmell', 'wuhan', 'lagosfoodrelief', 'setumerabodyguard', ' stayhome', 'cubasalvavidas', ' somoscuba', ' somoscontinuidad', 'fisa', ' r0', 'unibenvsunilag', ' ramadan', 'lockdown', 'np', 'coronaviruspandemic', ' coronavirusoutbreak', 'covidãƒ¼19', ' locusts', 'meawarenesshour', 'trump2020', 'singer', ' song', ' entertainment', ' perform', ' love', ' people', ' lockdown', 'gateshacked', 'earthtongues', 'covid_19sa', 'lionbonetrade', ' endwildlifemarkets', 'cryptofunds', 'nwilife', 'sexualviolence', ' domesticviolence', 'corono', 'macombcounty', 'coronavirusroundup', 'oann', 'letsmeetforabeer', 'africa', 'poverty', 'rcslt', ' slpeeps', 'kpop', ' kpopiscringe', 'pro', 'techblog', ' startup', 'mitchmcconnell', ' brazil', 'reopenvirginia', ' liberateminnesota', 'yourcsrd', ' heartmath', ' coherence', 'genc', 'ehsaastelethon', ' 23rdapril', 'community', ' hawkemedia', 'coronainpakistan', ' covid19vaccine', 'climateaction', 'coronavirusoutbreak', 'washtimesoped', 'fuckinguseless', ' trudeaudictatorship', 'migrants', 'onlineauction', ' onlinedonation', ' events', 'givingtuesdaynow', 'thejoieproject', 'higheredit', 'nyc', ' barber', ' relief', ' hedliash', ' fundingtweets', ' support', ' gofundme', ' plz', 'malliard', 'businessmorning', 'justdonated', 'isolation', ' fear', 'frontline', 'coronacrisis', 'oxygen', 'chromatica', 'herdimmunity']\n",
            "banyak jenis hashtags:  149\n",
            "banyak posts:  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create pair of two string\n",
        "def create_string_pair( selected_value, other_values ):\n",
        "  return [ ( selected_value, x ) for x in other_values[1:] ]\n",
        "# end func\n",
        "\n",
        "# get list of frequencies number between two hashtags\n",
        "def get_hashtags_frequencies( posts, hashtags_list ):\n",
        "  # initial value\n",
        "  final_pair_list = [] # with all created pairs\n",
        "  final_freq_list = [] # with all frequencies of each pair list\n",
        "  final_list_2 = [] # with index `hash 1` and values `hash 2, hash 3, hash 4, dst`\n",
        "\n",
        "  # create hashtags pair between selected pair and remaining hashtags\n",
        "  for i in range( len( hashtags_list ) ):\n",
        "    # print( i )\n",
        "    # bikin pasangannya\n",
        "    pairs = create_string_pair( hashtags_list[ i ], hashtags_list[i+1: ] )\n",
        "\n",
        "    # loop\n",
        "    for one_pair in pairs:\n",
        "      # divide both hashtags\n",
        "      hash_1 = one_pair[0]\n",
        "      hash_2 = one_pair[1]\n",
        "\n",
        "      # loop posts contains hashtags list\n",
        "      freq_counter = 0\n",
        "      for one_post in posts:\n",
        "        if ( hash_1 in one_post ) and ( hash_2 in one_post ):\n",
        "          freq_counter += 1\n",
        "\n",
        "          # # test\n",
        "          # print( one_post, \", -> \", hash_1, \" AND \", hash_2 )\n",
        "        # end if\n",
        "      # end loop posts\n",
        "\n",
        "      # only save freq with exist hashtags pairs\n",
        "      if freq_counter > 0:\n",
        "        final_pair_list.append( one_pair )\n",
        "        final_freq_list.append( freq_counter )\n",
        "      # end if\n",
        "\n",
        "    # end loop pairs\n",
        "\n",
        "  # end loop range list\n",
        "\n",
        "  # return final list\n",
        "  return final_pair_list, final_freq_list\n",
        "# end function\n",
        "\n",
        "def get_hashtags_pair_lists( posts, hashtags_list ):\n",
        "  # initial value\n",
        "  final_pair_list = [] # with all created pairs\n",
        "\n",
        "  # create hashtags pair between selected pair and remaining hashtags\n",
        "  for i in range( len( hashtags_list ) ):\n",
        "    # bikin pasangannya\n",
        "    pairs = create_string_pair( hashtags_list[ i ], hashtags_list[i+1: ] )\n",
        "\n",
        "    # loop\n",
        "    for one_pair in pairs:\n",
        "      # divide both hashtags\n",
        "      hash_1 = one_pair[0]\n",
        "      hash_2 = one_pair[1]\n",
        "\n",
        "      # loop posts contains hashtags list\n",
        "      # only save hashtags pairs that exists in posts\n",
        "      for one_post in posts:\n",
        "        if one_pair not in final_pair_list and ( hash_1 in one_post ) and ( hash_2 in one_post ):\n",
        "          final_pair_list.append( one_pair )\n",
        "        # end if\n",
        "      # end loop posts\n",
        "\n",
        "    # end loop pairs\n",
        "\n",
        "  # end loop range list\n",
        "\n",
        "  # return final list\n",
        "  return final_pair_list\n",
        "# end function\n",
        "\n",
        "# hashtags_on_posts --> hashtags data in form of string from each posts\n",
        "# varitions_of_hashtags --> hashtags variety from all posts\n",
        "# list_of_hashtags_from_each_post --> each hashtags list on each posts\n",
        "\n",
        "# list_edge_between_nodes, list_edges_weight = get_hashtags_frequencies( list_of_hashtags_from_each_post, varitions_of_hashtags )\n",
        "# print( \"list_edge_between_nodes: \", list_edge_between_nodes )\n",
        "# print( \"list_edges_weight: \", list_edges_weight )\n",
        "# print( \"length: \", len( list_edges_weight ) )\n",
        "\n",
        "\n",
        "list_edge_between_nodes = get_hashtags_pair_lists( list_of_hashtags_from_each_post, varitions_of_hashtags )\n",
        "print( \"list_edge_between_nodes: \", list_edge_between_nodes )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WgCJT6pa7v1",
        "outputId": "3f1f9efe-964d-4c6e-c540-14cd8cfd960b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list_edge_between_nodes:  [('covid_19', ' malware'), ('covid_19', ' cybersecurity'), ('covid_19', ' infosec'), ('covid_19', ' infosecurity'), ('covid_19', ' r0'), ('covid_19', ' heartmath'), ('covid_19', ' coherence'), ('coronavirus', ' covid19'), ('coronavirus', ' environment'), ('coronavirus', ' brazil'), ('covid19', ' medicareforall'), ('covid19', ' coronavirus'), ('covid19', ' demcastoh'), ('covid19', ' stayhome'), ('oic', ' india'), ('oic', ' covid'), (' islamophobic', ' covid'), (' india', ' covid_19'), (' india', ' stayhome'), (' india', 'singer'), (' india', ' song'), (' india', ' entertainment'), (' india', ' perform'), (' india', ' love'), (' india', ' people'), (' india', ' lockdown'), (' covid', ' covid19'), (' covid', 'live'), (' covid', 'cryptofunds'), (' covid', 'nyc'), (' covid', ' barber'), (' covid', ' relief'), (' covid', ' hedliash'), (' covid', ' fundingtweets'), (' covid', ' support'), (' covid', ' gofundme'), (' covid', ' plz'), (' covid', 'oxygen'), ('plasma', ' yoursafetyismysafety'), ('plasma', ' coronavirus'), ('plasma', ' covid_19'), ('plasma', ' qatar'), (' hmc', ' coronavirus'), (' hmc', ' covid_19'), (' hmc', ' qatar'), (' yoursafetyismysafety', ' covid_19'), (' yoursafetyismysafety', ' qatar'), (' coronavirus', ' qatar'), (' coronavirus', 'singapore'), (' coronavirus', 'isolation'), (' coronavirus', ' fear'), (' covid_19', ' stayhome'), (' covid_19', 'singer'), (' covid_19', ' song'), (' covid_19', ' entertainment'), (' covid_19', ' perform'), (' covid_19', ' love'), (' covid_19', ' people'), (' covid_19', ' lockdown'), (' covid_19', 'covid_19sa'), (' covid_19', 'community'), (' covid_19', ' hawkemedia'), (' covid_19', 'isolation'), (' covid_19', ' fear'), ('trump', ' covid__19'), ('trump', ' china'), (' covid19', 'cityofto'), (' covid19', 'lagos'), (' covid19', 'peanutbutter'), (' covid19', ' asymptomatic'), (' covid19', ' carriers'), (' covid19', ' covidtesting'), (' covid19', ' testing'), (' covid19', ' senseofsmell'), (' covid19', 'sexualviolence'), (' covid19', ' domesticviolence'), (' covid19', 'macombcounty'), (' covid19', ' brazil'), (' covid19', 'genc'), (' covid19', 'nyc'), (' covid19', ' barber'), (' covid19', ' relief'), (' covid19', ' hedliash'), (' covid19', ' fundingtweets'), (' covid19', ' support'), (' covid19', ' gofundme'), (' covid19', ' plz'), (' covid19', 'herdimmunity'), ('saheart', ' movemore'), (' covid__19', 'coronaviruspandemic'), (' malware', ' infosec'), (' malware', ' infosecurity'), (' cybersecurity', ' infosecurity'), ('peanutbutter', ' carriers'), ('peanutbutter', ' covidtesting'), ('peanutbutter', ' testing'), ('peanutbutter', ' senseofsmell'), (' asymptomatic', ' covidtesting'), (' asymptomatic', ' testing'), (' asymptomatic', ' senseofsmell'), (' carriers', ' testing'), (' carriers', ' senseofsmell'), (' covidtesting', ' senseofsmell'), (' stayhome', 'singer'), (' stayhome', ' song'), (' stayhome', ' entertainment'), (' stayhome', ' perform'), (' stayhome', ' love'), (' stayhome', ' people'), (' stayhome', ' lockdown'), ('cubasalvavidas', ' somoscontinuidad'), ('singer', ' entertainment'), ('singer', ' perform'), ('singer', ' love'), ('singer', ' people'), ('singer', ' lockdown'), (' song', ' perform'), (' song', ' love'), (' song', ' people'), (' song', ' lockdown'), (' entertainment', ' love'), (' entertainment', ' people'), (' entertainment', ' lockdown'), (' perform', ' people'), (' perform', ' lockdown'), (' love', ' lockdown'), (' lockdown', 'covid_19sa'), ('onlineauction', ' events'), ('nyc', ' relief'), ('nyc', ' hedliash'), ('nyc', ' fundingtweets'), ('nyc', ' support'), ('nyc', ' gofundme'), ('nyc', ' plz'), (' barber', ' hedliash'), (' barber', ' fundingtweets'), (' barber', ' support'), (' barber', ' gofundme'), (' barber', ' plz'), (' relief', ' fundingtweets'), (' relief', ' support'), (' relief', ' gofundme'), (' relief', ' plz'), (' hedliash', ' support'), (' hedliash', ' gofundme'), (' hedliash', ' plz'), (' fundingtweets', ' gofundme'), (' fundingtweets', ' plz'), (' support', ' plz')]\n"
          ]
        }
      ]
    }
  ]
}